\documentclass{article}

\usepackage[final]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Exploring the Evolution of Natural Language Processing until the Advent of the Transformer Model}


\author{%
  Kyeong-Hun Kim\\
  Course of NLP\\
  Aiffel Online-4\\
}


\begin{document}


\maketitle


\begin{abstract}
  
    The Transformer model has emerged as a groundbreaking architecture in the field of Natural Language Processing (NLP), revolutionizing language understanding, generation, and translation tasks. In this thesis, we delve into the Transformer's theoretical foundations, focusing on its key components such as self-attention, multi-head attention, positional encoding, position-wise feed-forward networks, and encoder-decoder architecture. We explore the reason for the Transformer's emergence and provide an overview of its approximate structure.

    While the Transformer's parallel processing and attention mechanisms offer substantial benefits, we also address its limitations, such as computational complexity for extremely long sequences and the challenge of deploying large models on resource-constrained devices. We investigate potential solutions and discuss directions for future research to enhance efficiency and address these limitations.
    
    Overall, this thesis provides a comprehensive understanding of the Transformer model's architecture, strengths, and limitations. It offers insights into its remarkable impact on NLP research and applications, guiding the way forward for harnessing the full potential of the Transformer model and pushing the boundaries of natural language processing.
\end{abstract}


\section{Introduction}


Natural Language Processing (NLP) has undergone remarkable advancements over the years, revolutionizing the way humans interact with machines and transforming various industries. From early rule-based systems to statistical methods and deep learning models, the field has continuously evolved to tackle complex language tasks. However, a significant breakthrough in NLP occurred with the introduction of the Transformer model, which has played a pivotal role in shaping the current state of NLP technology.

This thesis aims to delve into the fascinating journey of NLP development until the momentous arrival of the Transformer model. It traces the historical progression of NLP techniques and architectures, from the early attempts at language understanding to the emergence of deep learning methods that paved the way for the Transformer. The Transformer's innovative attention mechanism and self-attention mechanism have proven instrumental in achieving superior performance in various NLP tasks, and the impact of its introduction cannot be overstated.

Throughout this study, we will explore the key milestones and breakthroughs in NLP research that led up to the birth of the Transformer model. By examining the limitations of traditional NLP approaches, we will highlight the necessity for a new paradigm to address the challenges of long-range dependencies and capture global context efficiently. This investigation will also encompass the transformative influence of the Transformer on language modeling, machine translation, question-answering systems, and other essential NLP tasks.

The significance of understanding the historical context leading up to the Transformer's release lies in comprehending how the model revolutionized NLP research and applications. Additionally, we will analyze the subsequent advancements and refinements built upon the Transformer's foundation to appreciate the dynamic nature of the NLP landscape.

In conclusion, this thesis endeavors to shed light on the exciting journey of NLP advancements until the arrival of the Transformer model, unraveling the remarkable breakthrough that changed the landscape of NLP forever. By gaining a comprehensive understanding of the progress made "until the Transformer comes out," we can better grasp the implications of this milestone and the trajectory of NLP research that followed, setting the stage for future innovations and developments in this dynamic field.


\section{Background}


\subsection{Recurrent Neural Networks (RNN)}

Natural Language Processing (NLP) involves the development of algorithms and models that enable machines to understand, interpret, and generate human language. From sentiment analysis to machine translation and text generation, NLP plays a crucial role in various applications across industries. Recurrent Neural Networks (RNNs) have been at the forefront of NLP for quite some time due to their ability to capture sequential information in textual data.

RNNs are a class of neural networks designed to handle sequential data, making them particularly well-suited for processing natural language, which is inherently sequential in nature. Traditional feedforward neural networks process inputs independently, disregarding any inherent order or sequence in the data. However, RNNs address this limitation by introducing loops in their architecture, allowing information to persist across time steps.

The fundamental building block of an RNN is the recurrent unit, which can be visualized as a simplified neural network cell with two inputs: the current input and the output from the previous time step. This feedback loop allows the RNN to retain information about the past, making it capable of modeling temporal dependencies and context in a sequence of words.

In the context of NLP, RNNs have been widely used for various tasks, including:

1. Language Modeling: RNNs are used to predict the next word in a sequence based on the preceding words, effectively capturing the probability distribution of words in a language. Language models built with RNNs have been fundamental in improving tasks like text generation and speech recognition.

2. Sentiment Analysis: RNNs can analyze sentiment in textual data by understanding the sequential patterns of words that convey emotions or opinions. By capturing context, RNNs can better grasp the sentiment of a sentence or document.

3. Machine Translation: RNNs have been employed in sequence-to-sequence models, where the input sequence (source language) is mapped to an output sequence (target language). This enables the translation of text between different languages.

Despite their effectiveness in handling sequential data, traditional RNNs have some limitations, primarily related to the vanishing gradient problem. When training RNNs on long sequences, the gradients can become extremely small as they backpropagate through time, leading to difficulties in learning long-range dependencies.


\subsection{Sequence-to-Sequence (seq2seq)}

Sequence-to-Sequence (seq2seq) models have emerged as a groundbreaking approach in Natural Language Processing (NLP) that enables the processing of variable-length input sequences and produces variable-length output sequences. Introduced as a fundamental building block of machine translation systems, seq2seq models have since been applied to various NLP tasks, demonstrating their versatility and effectiveness.

At the core of a seq2seq model lies an encoder-decoder architecture. The encoder takes an input sequence, such as a sentence in a source language, and transforms it into a fixed-size representation known as a context vector or thought vector. The context vector captures the essential information and context of the input sequence, effectively compressing the input information into a fixed-length vector.

The decoder, on the other hand, takes the context vector and generates an output sequence, such as a translation in a target language or a response in a conversation. The decoder uses the context vector as an initial hidden state and generates each element of the output sequence one step at a time, taking into account the context and the previously generated elements.

The encoder-decoder framework is especially powerful for tasks like machine translation, where the length of the input and output sequences may vary. Traditional approaches in machine translation relied on statistical methods and rule-based systems, which struggled to capture long-range dependencies and produce fluent translations. Seq2seq models addressed these limitations by effectively handling variable-length input and output sequences, allowing for more natural and accurate translations.

One of the key innovations that paved the way for seq2seq models' success was the use of Recurrent Neural Networks (RNNs) as the basic building blocks of the encoder and decoder. RNNs' ability to handle sequential data and model temporal dependencies proved crucial in capturing context and information across the sequences.

Despite their effectiveness, traditional seq2seq models based on RNNs faced challenges when dealing with long sequences. The vanishing gradient problem, encountered during the backpropagation process, hindered learning of long-range dependencies and made it difficult to handle very long sentences or documents.


\subsection{Attention Mechanism}

The attention mechanism was developed to address the limitations of traditional sequence-to-sequence (seq2seq) models, which used Recurrent Neural Networks (RNNs) as their basic building blocks. While RNNs were effective in modeling sequential data, they suffered from the vanishing gradient problem when processing long sequences, leading to difficulties in capturing long-range dependencies and maintaining context over extended portions of the text.

The concept of attention, inspired by human cognitive processes, allows models to selectively focus on specific parts of the input sequence during processing. Instead of relying solely on the encoder's fixed-length context vector to represent the entire input sequence, attention enables the model to dynamically weigh the importance of different input elements based on their relevance to the current step of decoding or processing.

Attention mechanisms work by computing attention scores for each element in the input sequence relative to a particular query. These scores are then used to form a weighted sum of the input elements, producing a context vector that adapts to the specific context of the decoding step. By doing so, attention allows the model to emphasize relevant words or phrases and effectively capture long-range dependencies, making it well-suited for handling variable-length sequences.

In the context of NLP, attention has demonstrated its utility in various applications. In machine translation, attention-equipped seq2seq models have significantly improved translation quality by aligning source and target language words more effectively. In text generation tasks, attention helps models generate more coherent and contextually appropriate responses. Moreover, in question-answering systems and sentiment analysis, attention allows models to focus on the most salient information, leading to better performance.

\section{Transformer: Attention Is All You Need}


The Transformer model was developed to address the limitations of traditional sequence-to-sequence (seq2seq) models, which relied on recurrent neural networks (RNNs) as their fundamental building blocks. Although RNN-based seq2seq models demonstrated promising results in various natural language processing (NLP) tasks, they faced challenges with processing long sequences due to the vanishing gradient problem. This limitation hindered their ability to capture long-range dependencies effectively and maintain context over extended portions of the text.

The vanishing gradient problem occurs during the backpropagation process when gradients become extremely small as they propagate through time in RNNs. As a result, the model struggles to learn relationships between distant elements in the sequence, leading to suboptimal performance in tasks that require capturing global context and long-term dependencies. Consequently, this issue became a significant obstacle for RNN-based models in handling complex language tasks, such as machine translation, where the source and target sentences may be of variable lengths.

To overcome these limitations, the Transformer model introduced an innovative attention mechanism, which allowed it to process sequences in parallel and efficiently capture dependencies between all elements within the sequence. The attention mechanism enabled the model to focus on relevant parts of the input sequence, regardless of their positional distance, thereby addressing the vanishing gradient problem and facilitating the effective handling of long-range dependencies.


\subsection{Positional Encoding}


Positional encoding is a crucial component of the Transformer model, designed to provide information about the positional order of words in the input sequence without relying on recurrent or convolutional structures. In the Transformer, which operates on the principle of self-attention, positional encoding ensures that the model can effectively process sequential data while maintaining the advantages of parallel computation.

In traditional sequence-to-sequence models based on recurrent neural networks (RNNs), the sequential order of words is inherently preserved by the recurrence, as each time step corresponds to a specific position in the input sequence. However, the Transformer abandons recurrence and processes all words in parallel, making it essential to introduce positional information explicitly.

Positional encoding is added to the input embeddings of the Transformer, allowing the model to differentiate between words based on their positions in the sequence. This encoding is designed to be learned and added to the word embeddings during the model's training process. The positional encodings are generally fixed and do not depend on the specific input sentences, meaning that the same positional encoding is used for all sentences in a given task.

The most commonly used positional encoding technique in the Transformer model is based on the sine and cosine functions. Each dimension of the positional encoding corresponds to a sinusoidal curve, with the frequency increasing exponentially for different dimensions. The positional encoding for each word at position pos and each dimension i is calculated as follows:

$$ PE(pos, 2i) = \sin(pos / 10000^{2i/d_{model}}) $$
$$ PE(pos, 2i+1) = \cos(pos / 10000^{2i/d_{model}}) $$

Here, $pos$ represents the position of the word in the sequence, $i$ denotes the dimension of the positional encoding, and $d_{model}$ represents the dimensionality of the word embeddings.

The positional encoding is then added to the word embeddings, providing a unique representation for each word based on both its semantic meaning and its positional order in the sequence. As a result, the Transformer can capture the positional information without using recurrent connections, ensuring efficient parallel computation and enabling the model to handle longer sequences with ease.


\subsection{Multi-Head Attention}

Multi-Head Attention is a crucial component of the Transformer model that enhances the model's ability to capture complex relationships and dependencies within the input sequence. It extends the basic attention mechanism by employing multiple attention heads, each responsible for attending to different parts of the input sequence. The introduction of multi-head attention in the Transformer model has played a pivotal role in improving its performance on various natural language processing (NLP) tasks.

In the standard attention mechanism, a single attention head computes the attention scores between the query and each position in the input sequence, producing a context vector that summarizes the relevant information for the decoding or processing step. However, this single attention head may not fully capture the diverse dependencies and semantic relationships present in the sequence, especially when dealing with complex and nuanced language patterns.

Multi-Head Attention addresses this limitation by employing several attention heads in parallel. Each attention head independently learns its own set of attention weights and produces its context vector. These individual context vectors are then concatenated and linearly transformed to obtain the final output of the multi-head attention layer.

The primary advantages of multi-head attention are as follows:

1. Enhanced Expressiveness: By using multiple attention heads, the model can capture different types of dependencies and focus on various aspects of the input sequence simultaneously. This enables the model to represent a more diverse range of semantic relationships and context, making it more expressive and capable of handling complex language patterns.

2. Reduced Attention Bottlenecks: In the case of single-head attention, the attention computation may become a bottleneck for long sequences due to the quadratic complexity of attention calculations. Multi-head attention alleviates this issue by distributing the computation across multiple heads, making it more efficient and scalable for longer sequences.

3. Improved Generalization: Multi-head attention allows the model to jointly attend to multiple positions in the input sequence, facilitating a more holistic understanding of the context. This improved context modeling leads to better generalization, as the model can effectively capture long-range dependencies and global context in the data.

The multi-head attention mechanism is incorporated into both the encoder and the decoder of the Transformer model. In the encoder, multi-head attention enables the model to attend to various parts of the input sequence simultaneously, capturing different linguistic patterns and semantic relationships. In the decoder, multi-head attention plays a crucial role in attending to relevant parts of the encoder's output while generating the target sequence.


\subsection{Position-wise Feed-Forward Networks}

Position-wise Feed-Forward Networks (FFN) are an essential component of the Transformer model, contributing to its ability to capture complex non-linear relationships within the input sequence. Positioned after the self-attention mechanism in each encoder and decoder layer, the position-wise FFNs play a crucial role in transforming and refining the contextual representations of words in the sequence.

The position-wise FFN consists of two linear transformations followed by a non-linear activation function, typically a rectified linear unit (ReLU). It operates independently on each position in the input sequence, thus the term "position-wise." The FFN's design allows it to adaptively process different positions within the sequence, contributing to the model's capacity to model complex interactions and learn intricate patterns in the data.

The FFN can be mathematically represented as follows:

$$ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $$


$x$ is the input representation (e.g., the output of the self-attention mechanism). $W_1$ and $W_2$ are weight matrices learned during training. $b_1$ and $b_2$ are bias terms.

The first linear transformation $xW_1 + b_1$ projects the input representation to a higher-dimensional space, followed by the activation function ReLU, which introduces non-linearity to the model. The second linear transformation $max(0, xW_1 + b_1)W_2 + b_2$ educes the dimensionality of the representation back to the original size, thus completing the FFN operation.


\subsection{Additional Techniques}

\subsubsection{Layer Normalization}

Layer normalization is a normalization technique applied to the output of each sublayer within the encoder and decoder stacks of the Transformer. It addresses the issue of internal covariate shift, ensuring more stable and efficient training. Layer normalization normalizes the activations of each layer independently, resulting in faster convergence and improved generalization. The application of layer normalization in the Transformer contributes to its robustness and the ability to handle various data distributions effectively.

\subsubsection{Residual Connections}

Residual connections, also known as skip connections, play a vital role in the Transformer model's architecture. These connections allow the output of each sublayer to bypass the sublayer and be directly added to the sublayer's input. This design choice facilitates smooth gradient flow during training, which helps to mitigate the vanishing gradient problem and stabilizes the training process. Residual connections enable the model to effectively learn complex representations by building on the existing features from the preceding layers, leading to improved optimization and convergence.

\subsubsection{Encoder-Decoder Architecture}

The Transformer model employs an encoder-decoder architecture for tasks such as machine translation. The encoder processes the input sequence, encoding it into a fixed-length context vector, while the decoder generates the output sequence, conditioned on the context vector and the previously generated elements. The encoder-decoder architecture facilitates the handling of variable-length input and output sequences, making it well-suited for sequence-to-sequence tasks.

\subsubsection{Label Smoothing}

In certain tasks, such as machine translation, label smoothing is employed to prevent the model from becoming overly confident in its predictions. Label smoothing replaces the hard target labels with a smoothed distribution that assigns low probabilities to the true targets and redistributes the remaining probability mass to other candidate tokens. This regularization technique prevents the model from becoming too certain about its predictions, leading to improved generalization and better performance on unseen data.

Incorporating these important components into the methods section of the thesis will provide a comprehensive understanding of the complete Transformer architecture and its various features. The combination of self-attention, multi-head attention, positional encoding, position-wise feed-forward networks, layer normalization, residual connections, and encoder-decoder architecture makes the Transformer a versatile and powerful model for natural language processing tasks, revolutionizing the field and setting new standards for NLP research and applications.


\section{Conclusion}


The Transformer model has undoubtedly revolutionized the field of natural language processing (NLP) and set new standards for language understanding, generation, and translation tasks. As we conclude this thesis, we look towards the future and identify promising directions of development based on the Transformer model, aiming to push the boundaries of NLP research and applications further.

\subsection{Enhanced Efficiency for Long Sequences}

The computational complexity of the Transformer model still poses challenges when dealing with extremely long sequences. Future research should focus on devising more efficient attention mechanisms or architectures to handle such sequences without sacrificing performance. Techniques like sparse attention and hierarchical attention might offer potential solutions to address the computational burden associated with long sequences.

\subsection{Hybrid Models}

Combining the strengths of different architectures could yield even more powerful models for NLP tasks. Exploring hybrid models that integrate the Transformer with other models, such as convolutional neural networks (CNNs) or graph neural networks (GNNs), might lead to novel approaches that leverage both sequential and hierarchical information in textual data.

\subsection{Dynamic Context Adaptation}

Current Transformer models rely on fixed-length context vectors, which may not be optimal for all tasks. Investigating dynamic context adaptation mechanisms that allow the model to adapt the context size based on the complexity and length of the input sequence could further improve its performance on different tasks.

\subsection{Multi-Modal Transformers}

Extending the Transformer model to handle multi-modal inputs, such as combining textual and visual information, holds great potential for tasks like image captioning and visual question answering. Developing multi-modal Transformer architectures could enable more comprehensive understanding and generation of content in diverse data types.

\subsection{Explainable Transformers}

Interpreting and understanding the decisions made by Transformer models remain challenging due to their black-box nature. Exploring techniques for making Transformer models more explainable and interpretable could enhance their applicability in critical domains, such as healthcare and finance.

\subsection{Transformers for Low-Resource Languages}

The power of pre-trained Transformers lies in their ability to transfer knowledge from large-scale data. Future research should focus on adapting and fine-tuning pre-trained Transformers for low-resource languages, enabling the benefits of this technology to extend to a broader range of linguistic communities.

\subsection{Robustness and Bias Mitigation}

Addressing the issue of bias in NLP models and enhancing their robustness to adversarial attacks are critical directions for future research. Developing techniques to detect and mitigate bias in Transformers, as well as enhancing their resilience to malicious input, will be crucial for ensuring the ethical and responsible deployment of NLP models.


In conclusion, the Transformer model has marked a paradigm shift in NLP, and its potential for future development is vast. By addressing its current limitations and exploring new frontiers, we can unlock even greater capabilities, leading to more sophisticated and versatile models. The journey from the Transformer to its future iterations promises to shape the landscape of NLP research, opening up exciting possibilities for real-world applications and advancing our understanding of human language interaction with machines. As we embark on this transformative path, we remain dedicated to pushing the boundaries of NLP and harnessing the full potential of the Transformer model for the benefit of society.


\section*{References}


{
\small


[1] Mikolov, T., Karafiát, M., Burget, L., Cernocký, J., \& Khudanpur, S. (2010, September). Recurrent neural network based language model. In Interspeech (Vol. 2, No. 3, pp. 1045-1048).

[2] Sutskever, I., Vinyals, O., \& Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.


[3] Bahdanau, D., Cho, K., \& Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

[4] Luong, M. T., Pham, H., \& Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.

[5] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

}


\end{document}
