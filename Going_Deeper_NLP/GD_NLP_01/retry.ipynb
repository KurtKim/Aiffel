{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee8ac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table('./data/ratings.txt')\n",
    "\n",
    "df = df.dropna(how='any')\n",
    "df = df.drop_duplicates(['document'])\n",
    "\n",
    "df = df[df['document'].apply(lambda it: len(it) >= 10)]\n",
    "df = df[df['document'].apply(lambda it: len(it) <= 70)]\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95958329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in df['document']:\n",
    "        f.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333051f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=retry --model_type=unigram --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: retry\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 159645 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4761577\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1731\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 159645 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 273519 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 159645\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 314733\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 314733 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=139892 obj=15.3082 num_tokens=742984 num_tokens/piece=5.31113\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=129680 obj=14.2331 num_tokens=747444 num_tokens/piece=5.76376\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=97212 obj=14.3303 num_tokens=779416 num_tokens/piece=8.01769\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=97051 obj=14.2746 num_tokens=779816 num_tokens/piece=8.03512\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=72784 obj=14.5128 num_tokens=819217 num_tokens/piece=11.2555\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=72778 obj=14.4501 num_tokens=819309 num_tokens/piece=11.2576\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54583 obj=14.7148 num_tokens=856991 num_tokens/piece=15.7007\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=54583 obj=14.6523 num_tokens=856981 num_tokens/piece=15.7005\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=40937 obj=14.9514 num_tokens=897808 num_tokens/piece=21.9315\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=40937 obj=14.8863 num_tokens=897827 num_tokens/piece=21.9319\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30702 obj=15.2156 num_tokens=939411 num_tokens/piece=30.5977\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=30702 obj=15.148 num_tokens=939397 num_tokens/piece=30.5973\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=23026 obj=15.5118 num_tokens=982611 num_tokens/piece=42.674\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=23026 obj=15.4376 num_tokens=982665 num_tokens/piece=42.6763\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17269 obj=15.8332 num_tokens=1027723 num_tokens/piece=59.5126\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17269 obj=15.7509 num_tokens=1027735 num_tokens/piece=59.5133\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=12951 obj=16.1877 num_tokens=1075861 num_tokens/piece=83.0717\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=12951 obj=16.0949 num_tokens=1075935 num_tokens/piece=83.0774\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=9713 obj=16.5763 num_tokens=1128732 num_tokens/piece=116.208\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=9713 obj=16.4615 num_tokens=1128929 num_tokens/piece=116.229\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=16.6312 num_tokens=1147685 num_tokens/piece=130.419\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=16.5939 num_tokens=1147839 num_tokens/piece=130.436\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: retry.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: retry.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --model_type={} --vocab_size={}'.format(\n",
    "        temp_file, 'retry', 'unigram', 8000)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31afddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def sp_tokenize(s, corpus, spm):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(f'{spm}.vocab', 'r') as f: \n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1507bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 8)           64000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                18688     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 82,753\n",
      "Trainable params: 82,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('retry.model')\n",
    "\n",
    "tensor, word_index, index_word = sp_tokenize(s, df['document'], 'retry')\n",
    "    \n",
    "x_train, x_val, y_train, y_val = train_test_split(tensor, df['label'], test_size=0.2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "word_vector_dim = 8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(8000, word_vector_dim))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794e068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1597/1597 [==============================] - 42s 6ms/step - loss: 0.4162 - accuracy: 0.8057 - val_loss: 0.3592 - val_accuracy: 0.8419\n",
      "Epoch 2/20\n",
      "1597/1597 [==============================] - 9s 5ms/step - loss: 0.3294 - accuracy: 0.8580 - val_loss: 0.3564 - val_accuracy: 0.8463\n",
      "Epoch 3/20\n",
      "1597/1597 [==============================] - 9s 5ms/step - loss: 0.3142 - accuracy: 0.8649 - val_loss: 0.3481 - val_accuracy: 0.8478\n",
      "Epoch 4/20\n",
      "1597/1597 [==============================] - 9s 6ms/step - loss: 0.2977 - accuracy: 0.8729 - val_loss: 0.3396 - val_accuracy: 0.8500\n",
      "Epoch 5/20\n",
      "1597/1597 [==============================] - 9s 6ms/step - loss: 0.2721 - accuracy: 0.8837 - val_loss: 0.3419 - val_accuracy: 0.8522\n",
      "Epoch 6/20\n",
      "1597/1597 [==============================] - 9s 6ms/step - loss: 0.2444 - accuracy: 0.8970 - val_loss: 0.3558 - val_accuracy: 0.8486\n",
      "Epoch 7/20\n",
      "1597/1597 [==============================] - 9s 5ms/step - loss: 0.2213 - accuracy: 0.9071 - val_loss: 0.3681 - val_accuracy: 0.8432\n",
      "Epoch 8/20\n",
      "1597/1597 [==============================] - 9s 5ms/step - loss: 0.1978 - accuracy: 0.9181 - val_loss: 0.4185 - val_accuracy: 0.8400\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "epochs=20\n",
    "batch_size=64\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "    validation_data=(x_val,y_val), callbacks=es, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb3d244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42206740379333496, 0.8391402959823608]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
